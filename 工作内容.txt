sklearn
http://blog.csdn.net/lihaitao000/article/details/66972039

支持向量机
NB  是 朴素贝叶斯， KNN 是K近邻。SVM是支撑向量机，GBDT 是一种决策树
朴素贝叶斯讲解：https://www.cnblogs.com/nxld/p/6607943.html

3月20日
学习编码算法
比如tfidf、one-hot

FEATURE_ORDERED.txt
这是我用信息增益方法对1-gram特征做的排序，越靠前的特征，信息增益越大。
也就代表了用这个特征更容易区分 合法弹幕 和 非法弹幕。

pass.log
合法弹幕

reject.log
非法弹幕

可以从feature中选取前1000个 或者前2000个（你可以试一下怎么样效果最好），
使用VSM（Vector Space Model） 把特征映射成为向量，然后使用sklearn 中的分类函数做train模型

最简单的方法是：如果你选取了1000个特征， 那么，每条弹幕就是一个1000维的向量，
每一维代表一个特征，如果弹幕包含这个字，那么这一维就是1 否则是0

稍微进阶一些的方法是：如果你选取了1000个特征， 那么，每条弹幕就是一个1000维的向量，
每一维代表一个特征，如果弹幕包含这个字，那么这一维就是一个大于0小于1的值
（不同特征这个值可能不同，根据出现的频次得到），否则是0

这篇文档和我们的任务有些像http://www.xzbu.com/4/view-6759873.htm

VSM主要是为了把 你的句子转化为 数值化向量 ，从而可以通过各种模型去做判别。

建议你可以尝试三个方法

第一个是词袋模型， 每个句子用一个向量表示，
向量中每一位代表一个字，如果句子中包含这个字，
相应的位置置为1，否则为0

第二个是基于权重的模型，与词袋模型类似，
但是词是否出现不用简单的1，0来划分，而是用词出现的频率来设置。

第三个是tf-idf模型，你应该之前看了不少了
如果这些尝试完了，你还可以尝试下word2vec，这是神经网络的内容了
嗯嗯  信息熵那个文件里面是通过对每个字的熵排序进行得到的

越在前面的熵也越大，特征越值得考虑
因为所有特征都考虑了会使得向量维度太大，
所以需要提取一部分特征来表示整个句子
严格来说 是信息增益,信息增益越大，  证明特征越有用
一会我会重新整一下数据，分出训练集和测试集
你可以再训练集上做训练，通过测试集判断性能

tfidf < tf < wf

word2vec_model 
这个东西  是我训练好的word2vec，
 就是每个单词对应一个vector。
 你用gensim.models.Word2Vec.load(path)读取就可以了
 返回一个类似于dict的东西，你想查每个字的向量，直接用dict["XX"]就行
这个你也可以当做一种编码方式，句子的编码用句子中所有词的vector的平均值就可以。

################
http://www.runoob.com/python/python-gui-tkinter.html
做个小界面出来
Tkinter这个库还是很好上手的
没事的话可以试着弄弄  这个不着急   师姐不一定会让做这个  
不过就是以后你想用python做项目的时候  用UI前端的东西比较适合展示
Tkinter  或者 wxpython 你看看喜欢哪个用哪个就行


################
5月15
dfa.py
这是dfa算法 使用规则来过滤违规弹幕
需要load敏感词表
敏感词表还没晒好
筛好
弄好后给你
你可以先熟悉一下代码
sortfeature.py这是对特征排序的代码
目前只实现了信息增益1-gram的
之后我把分词后的数据给你
你可以先在服务器上 word_level的1gram的特征排序
之前给你的 char level的 1-gram的  特征  就是用这个程序做的

这是分词后的数据
你可以先去给特征排序

###############################
5月18日



colab  免费服务器
